
Scenario Based questions:

Will the reducer work or not if you use “Limit 1” in any HiveQL query?
ans: Depends on complexity of the query for direct queries the reducer wont be used but incase of a complex query it could use.

Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
ans: When multiple clients try to access the default metastore it will throw error as it allows only one hive session at a time.

Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
ans: The current table could have latency as it is not stored in an orc or parquet format further could be partioned for each month dynamically which takes less computing time. so we could follow below steps
* Create a table which stores data in an orc format.
CREATE TABLE transaction_details_orc
(
 cust_id INT, 
 amount FLOAT, 
 month STRING, 
 country STRING
 ) 
 stored as orc;
* create a table for dynamically partioning for the field month
CREATE TABLE partitioned_transaction 
(
cust_id INT, 
amount FLOAT, 
country STRING
) 
PARTITIONED BY (month STRING) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ‘,’ ;
* Enable dynamic partitioning properties as below
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
* Transfer data from orc to partition table 
INSERT OVERWRITE TABLE partitioned_transaction PARTITION (month) SELECT cust_id, amount, country, month FROM transaction_details_orc;

The above steps will partition the table with respect to each month and thus could be easier and faster to compute total revenue generated for each month.

How can you add a new partition for the month December in the above partitioned table?
ans: ALTER TABLE pratitioned_transaction ADD PARTITION(month="DEC") LOCATION '/location in hdfs';


I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
ans: By enabling below properties
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
CREATE TABLE sample
(
id int, 
first_name string, 
last_name string, 
email string,
gender string, 
ip_address string
) 
ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.OpenCSVSerde’ 
STORED AS TEXTFILE;


Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
ans: One can use the SequenceFile format which will group these small files together to form a single sequence file. 

* Create a temporary table:
CREATE TABLE temp_table (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS TEXTFILE;
* Load the data into temp_table:
LOAD DATA INPATH ‘/inputdir’ INTO TABLE temp_table;
* Create a table that will store data in SequenceFile format:
CREATE TABLE sample_seqfile (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS SEQUENCEFILE;
* Transfer the data from the temporary table into the sample_seqfile table:
INSERT OVERWRITE TABLE sample_seqfile SELECT * FROM temp_table;


LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?
ans: Since the file path is from the local it should be initialized with complete path location as file:///

Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?




Hive Practical questions:

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

ans: 
create table customers
(
id int,
name string,
age int,
address string,
salary int
)
row format delimited
feilds terminated by ',';

create table orders
(
oid int,
date string,
customer_id int,
amt int
)
row format delimited
feilds terminated by ',';

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

ans:
* inner join
select * from customer c
inner join orders o
on c.id = o.customer_id

* left join
select * from customer c
left join orders o
on c.id = o.customer_id

* right join
select * from customer c
right join orders o
on c.id = o.customer_id

* full outer join
select * from customer c
full outer join orders o
on c.id = o.customer_id

BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset 
ans:
create table AIRQUALITY
(
da_te string,
ti_me string,
CO_GT float,
PT08_S1_CO int,
NMHC_GT int,
C6H6_GT float,
PT08_S2_NMHC int,
NOx_GT int,
PT08_S3_NOx int,
NO2_GT int,
PT08_S4_NO2 int,
PT08_S5_O3 int,
T float,
RH float,
AH float
)
row format delimited
fields terminated by ','
tblproperties("skip.header.line.count"="1");

2. try to place a data into table location
ans:
 load data local inpath 'file:///Users/arnol/Downloads/AirQualityUCI/AirQualityUCI.csv' into table AIRQUALITY;
 
3. Perform a select operation . 
ans: 
 select * from AIRQUALITY limit 5;

4. Fetch the result of the select operation in your local as a csv file . 
ans: 
 creating csv file in hadoop:
 INSERT OVERWRITE DIRECTORY '/tmp/hive/airquality_tocsv.csv' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' SELECT *
 FROM AIRQUALITY_tocsv;
 
5. Perform group by operation . 
ans: 
 select da_te,sum(CO_GT) from AIRQUALITY group by da_te;

7. Perform filter operation at least 5 kinds of filter examples . 
ans: 
1) select * from AIRQUALITY where da_te = '10-03-2004';

2) select * from AIRQUALITY where da_te = '10-03-2004' and time > '20:00:00';

3) select * from AIRQUALITY where da_te = '10-03-2004' limit 3;

4) select * from AIRQUALITY where da_te in ('10-03-2004', '11-03-2004');

5) select da_te,count(CO_GT) from AIRQUALITY where da_te = '31-04-2005' group by da_te;

8. show and example of regex operation
ans: 
 select distinct(da_te) from AIRQUALITY where da_te regexp '^01';
 
9. alter table operation 
10 . drop table operation
12 . order by operation . 
13 . where clause operations you have to perform . 
14 . sorting operation you have to perform . 
15 . distinct operation you have to perform . 
16 . like an operation you have to perform . 
17 . union operation you have to perform . 
18 . table view operation you have to perform . 



hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations
